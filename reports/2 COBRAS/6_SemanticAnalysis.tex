When analyzing students' collaborative behaviour, the meaning of what they are writing can give us very useful insights. In order to apply machine learning algorithms to obtain these insights into the semantic information of the texts or in order to project the text into a latent d-dimensional space we need to use semantic analysis.
 \subsection{Pre-trained models}
 \label{sub:pretrained}
 Due to the relatively small size of our data, as well as computational constraints and time limitations, it would be impossible for us to build a good performance model. However, there are many pre-trained models available, which have already been trained on large datasets and make semantic analysis faster and less expensive (data and computation-wise).
 
 There are several pre-trained models available online that have been shared by different Machine Learning research groups. We chose three different pre-trained models and compare their performance on our dataset.
 
 \subsubsection{doc2vec}
 Le and Mikolov proposed doc2vec as a extension to word2vec to learn document-level embeddings \cite{doc2vec}. Then, Jey Han and Timoth verified doc2vec model's performance by comparing it to two baselines and two state-of-the-art document embedding methodologies and shared their pre-trained models \cite{doc2vecPretrained}. We downloaded two pre-trained doc2vec (DBOW method) models that have been trained on the English Wikipedia and Associated Press News websites respectively. 
\subsubsection{sent2vec}
A novel, computationally efficient, unsupervised, C-BOW-inspired method was introduced by Matteo and approach for training and inferring sentence embedding\cite{sen2vec}. We use two of their pre-trained sent2vec models, which were trained on the English Wikipedia and Toronto books respectively.
\subsubsection{Spacy}
Spacy is an open-source python library for industrial-strength natural language processing. It offers several pre-trained models such as \textit{en\_core\_web\_lg}, which is an English multitask CNN trained on OntoNotes, with GloVe vectors trained on Commom Crawl.
\subsection{Test Dataset}
In order to compare the performance of each pre-trained model on our dataset, we manually select some data as the test dataset and split it into three groups of text pairs depending on whether we considered them to be similar, medium similar or non-similar to each other. The text pairs of similar data were created by translating English text into Chinese and then translating Chinese to English. As for non-similar data and medium similar data, the text pairs are selected from each document of our dataset.
\subsection{Data Preprocessing}
There are many characters and groups of characters that are useless for semantic analysis such as punctuation, whitespace, very common words, numbers, URLs and so on in the text. Function \texttt{cleanedText} removes common words (Stoplist in Spacy), punctuation characters, numbers and URLs, then changes everything to lowercase and stores the prototype of each word. In addition, we defined different text processes that were applied to the text after our first processing, that were necessary for using the pre-trained models afterwards because of each pre-trained model's special requirements.
\subsection{Text Recovery}
\label{sub:textRecovery}
Recovering text based on \texttt{WindowOperation}, \texttt{SuperParagraph} or \texttt{Pad} is the most essential step before we do semantic analysis and we will use recovery text in \texttt{WindowOperation} as expample in following introduction. Recovering text is quite difficult in \texttt{WindowOperation} since we separated Operations by authors when creating \texttt{WindowOperations} from \texttt{Pad}, and collaborators' interaction information between cannot be retained. For example, when user1 wrote `year' in one \texttt{Operation} and user2 found this word should be plural form and then user2 add a \texttt{Operation} which added `s' at the end of `year'. This interaction is ignored when we only record one author's \texttt{Operations} in one \texttt{WindowOperation}. Fortunately, this kind of situation only happened by chance and it does not have a great impact on sentence embedding when we have enough words in one \texttt{WindowOperation}. In the beginning we tried to recovery text by \texttt{ElementaryOperation}, in which we needed to real-time trace the position of each \texttt{ElementaryOperation}. And tracing \texttt{ElementaryOperations}' position is not easy since you do not have completed position change information.  For example, the recovered text will be wrong in use1's \texttt{WindowOperation} when use2 modified(add or delete) a letter in the medium of text, since the \texttt{ElementaryOperations}' position is based on total \texttt{Pad} and user1 do not have user2's \texttt{ElementaryOperations}' information when two users \texttt{ElementaryOperation} are in different \texttt{WindowOpeartions}. Finally we tried to recover text base on \texttt{Operation} since we can obtain completed words from each \texttt{Operation}. We created a function \texttt{getOpText} to recover text and then sorted the \texttt{Operations} by start time and recover \texttt{WindowOperation}'s text by \texttt{Operation}'s text and its position. As for \texttt{Superparagraph}, function \texttt{pad\_at\_timestamp} creates a copy of a pad until a specific time and we can then retrieve the \texttt{Pad}'s text and separate it to obtain the superparagraphs' texts. Function \texttt{PreprocessOperationByAuthor} groups the \texttt{Operations} by authors and recovers their text, to see the similarity between different users in the total \texttt{Pad}.

\subsection{Similarity of WindowOperation and SuperParagraph}
After retrieving the text to be analyzed and choosing the  pre-trained model we can carry out the text analysis. We use cosine similarity between two vectors of texts, which is one of the most commonly used metrics for measuring similarity. When there is only one valid text in the pair, which means that there is only one non-zero text vector, we set the similarity value to 0. We apply the similarity metric on \texttt{WindowOperation} in order to find whether collaborators' writing topics tend to be gradually more similar. We will also look at the comparisons between \texttt{SuperParagraphs}.
\label{sub:similarity}
